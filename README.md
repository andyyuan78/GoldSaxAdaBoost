GoldSaxAdaBoost
===============

Modelled, Architected and designed by Vance King Saxbe. A. with the geeks from GoldSax Consulting, GoldSax Money, GoldSax Treasury, ,  and GoldSax Technologies email @vsaxbe@yahoo.com. Development teams from Power Dominion Enterprise, Precieux Consulting. This Engagement sponsored by GoldSax Foundation, GoldSax Group and executed by GoldSax Manager. By Ensemble methods, Combining multiple classifiers exploits the shortcomings of single classifiers, such as overfitting. Combining multiple classifiers help, as long as the classifiers are significantly different from each other. This difference is in the algorithm or in the data applied to that algorithm. The two types of ensemble methods bagging and boosting. In bagging, datasets the same size as the original dataset are built by randomly sampling examples for the dataset with replacement. Boosting takes the idea of bagging a step further by applying a different classifier sequentially to a dataset. AdaBoost uses a weak learner as the base classifier with the input data weighted by a weight vector. In the first iteration the data is equally weighted. But in subsequent iterations the data is weighted more strongly if it was incorrectly classified previously. This adapting to the errors is the strength of AdaBoost. We built functions to create a classifier using AdaBoost and the weak learner, decision stumps. The AdaBoost functions can be applied to any classifier, as long as the classifier can deal with weighted data. The AdaBoost algorithm is powerful, and it quickly handled datasets that were difficult using other classifiers. The classification imbalance problem is training a classifier with data that doesnâ€™t have an equal number of positive and negative examples. The problem also exists when the costs for misclassification are different from positive and negative examples. We looked at ROC curves as a way to evaluate different classifiers. We use precision and recall as metrics to measure the performance classifiers when classification of one class is more important than classification of the other class. We use oversampling and undersampling as ways to adjust the positive and negative examples in a dataset.dealing with classifiers with unbalanced objectives. This method takes the costs of misclassification into account when training a classifier. 
